functionimport numpy as np# -----------------------------# Step 1: Input and Output# -----------------------------# XOR input and expected outputX = np.array([ [0, 0], [0, 1], [1, 0], [1, 1]])y = np.array([[0], [1], [1], [0]])# -----------------------------# Step 2: Initialize Parameters# -----------------------------np.random.seed(42)input_neurons = 2hidden_neurons = 2output_neurons = 1learning_rate = 0.5epochs = 10000
# Random weight initializationW1 = np.random.uniform(size=(input_neurons, hidden_neurons))b1 = np.random.uniform(size=(1, hidden_neurons))W2 = np.random.uniform(size=(hidden_neurons, output_neurons))b2 = np.random.uniform(size=(1, output_neurons))# -----------------------------# Step 3: Activation Functions# -----------------------------def sigmoid(x): return 1 / (1 + np.exp(-x))def sigmoid_derivative(x): return x * (1 - x)# -----------------------------# Step 4: Training Loop (Forward + Backward Propagation)# -----------------------------for epoch in range(epochs): # ---- Forward pass ---- hidden_input = np.dot(X, W1) + b1 hidden_output = sigmoid(hidden_input) final_input = np.dot(hidden_output, W2) + b2 final_output = sigmoid(final_input) # ---- Compute Error ---- error = y - final_output # ---- Backpropagation ---- d_final = error * sigmoid_derivative(final_output) d_hidden = d_final.dot(W2.T) * sigmoid_derivative(hidden_output) # ---- Update Weights ---- W2 += hidden_output.T.dot(d_final) * learning_rate b2 += np.sum(d_final, axis=0, keepdims=True) * learning_rate W1 += X.T.dot(d_hidden) * learning_rate b1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate # Optionally print loss every 1000 epochs if epoch % 1000 == 0: loss = np.mean(np.square(error)) print(f"Epoch {epoch}: Loss = {loss:.5f}")# -----------------------------# Step 5: Final Output# -----------------------------print("\n=== Final Outputs After Training ===")for i in range(len(X)): print(f"Input: {X[i]} -> Output: {final_output[i][0]:.4f}")
Epoch 0: Loss = 0.32466Epoch 1000: Loss = 0.01257Epoch 2000: Loss = 0.00255Epoch 3000: Loss = 0.00134Epoch 4000:
